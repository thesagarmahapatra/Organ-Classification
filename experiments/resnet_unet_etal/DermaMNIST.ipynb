{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyEQr1OeQa6e",
    "outputId": "89acf43c-fd26-49a5-8db8-531f6dbe79ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Connect to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "6w5EVGTAPAhL",
    "outputId": "4c585c72-0101-4519-c710-aa3ed8a603c0"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7e509525f068>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Extract the arrays from the .npz file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mval_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAGIC_PREFIX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 return format.read_array(bytes,\n\u001b[0m\u001b[1;32m    257\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    833\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m   1008\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Generate image folder and test, val and train csvs \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = '/content/drive/MyDrive/DermaMNIST/dermamnist_128.npz'\n",
    "data = np.load(dataset_path)\n",
    "\n",
    "train_images = data['train_images']\n",
    "train_labels = data['train_labels']\n",
    "val_images = data['val_images']\n",
    "val_labels = data['val_labels']\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']\n",
    "\n",
    "def save_image_array(image_array, output_path):\n",
    "    image = Image.fromarray(image_array.astype('uint8'))\n",
    "    image.save(output_path)\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/DermaMNIST/images'\n",
    "\n",
    "def create_image_file_paths(images, labels, dataset_type):\n",
    "    image_paths = []\n",
    "    label_list = []\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        image_filename = f'{dataset_type}_image_{i}.png'\n",
    "        image_filepath = os.path.join(output_dir, image_filename)\n",
    "\n",
    "        save_image_array(image, image_filepath)\n",
    "\n",
    "        image_paths.append(image_filepath)\n",
    "        label_list.append(label)\n",
    "\n",
    "    return image_paths, label_list\n",
    "\n",
    "train_image_paths, train_labels_list = create_image_file_paths(train_images, train_labels, 'train')\n",
    "val_image_paths, val_labels_list = create_image_file_paths(val_images, val_labels, 'val')\n",
    "test_image_paths, test_labels_list = create_image_file_paths(test_images, test_labels, 'test')\n",
    "\n",
    "train_df = pd.DataFrame({'image_path': train_image_paths, 'label': train_labels_list})\n",
    "val_df = pd.DataFrame({'image_path': val_image_paths, 'label': val_labels_list})\n",
    "test_df = pd.DataFrame({'image_path': test_image_paths, 'label': test_labels_list})\n",
    "\n",
    "train_df.to_csv('train_dataset.csv', index=False)\n",
    "val_df.to_csv('val_dataset.csv', index=False)\n",
    "test_df.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "combined_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "print(\"Images saved and CSV files have been created for train, validation, and test datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UbP3NZSSyt9",
    "outputId": "92ea3d92-c09e-43af-f806-8808d511c3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.26.2)\n",
      "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (1.16.0)\n",
      "Collecting timm==0.9.7 (from segmentation-models-pytorch)\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.6)\n",
      "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (6.0.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation-models-pytorch) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24.6->segmentation-models-pytorch) (2024.8.30)\n",
      "Downloading segmentation_models_pytorch-0.3.4-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=cdf10f62e1fd64ac0fe6850b4ffd4fbac282d3edeac4d39ed0938bb71525d042\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=550cff76265920b66528dd9d28fdc8b62b4b00fdd74fde42f7039ba242eea671\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 1.0.11\n",
      "    Uninstalling timm-1.0.11:\n",
      "      Successfully uninstalled timm-1.0.11\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.4 timm-0.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision segmentation-models-pytorch pandas pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F24z8HAnY7eR",
    "outputId": "d7ac6e93-cb74-4364-829d-7e7c9aae9af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python pandas pillow numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxmuwTO4Y6x_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Wrhd-KwhwXx",
    "outputId": "288bb7fe-7f80-410d-ea4a-9bc5cf1661b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: /content/test_dataset.csv\n",
      "File not found: /content/val_dataset.csv\n",
      "File not found: /content/train_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# To move csv files from local /content to specific Drive\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "source_dir = \"/content/\"  \n",
    "\n",
    "destination_dir = \"/content/drive/MyDrive/DermaMNIST/\"\n",
    "os.makedirs(destination_dir, exist_ok=True)  \n",
    "\n",
    "csv_files = [\"test_dataset.csv\", \"val_dataset.csv\", \"train_dataset.csv\"]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    source_path = os.path.join(source_dir, csv_file)\n",
    "    destination_path = os.path.join(destination_dir, csv_file)\n",
    "\n",
    "    if os.path.exists(source_path):  \n",
    "        shutil.move(source_path, destination_path)\n",
    "        print(f\"Moved {csv_file} to {destination_dir}\")\n",
    "    else:\n",
    "        print(f\"File not found: {source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "tWiJIk1MXprG",
    "outputId": "6a1f64a3-fc5e-4847-e1e3-3d5647064eba"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6fdaa6bcbd65>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Process train, val, and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mprocess_dataset_with_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_updated_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mprocess_dataset_with_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_updated_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprocess_dataset_with_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_updated_csv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6fdaa6bcbd65>\u001b[0m in \u001b[0;36mprocess_dataset_with_masks\u001b[0;34m(csv_path, updated_csv_path, image_dir, mask_dir, threshold)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Generate masks for all images in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     df['mask_path'] = df['image_path'].apply(\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerate_mask_with_opencv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6fdaa6bcbd65>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Generate masks for all images in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     df['mask_path'] = df['image_path'].apply(\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mlambda\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerate_mask_with_opencv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6fdaa6bcbd65>\u001b[0m in \u001b[0;36mgenerate_mask_with_opencv\u001b[0;34m(image_path, threshold, mask_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Load the image in grayscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Image not found: {image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add Mask Path to csvs\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "image_dir = \"/content/drive/MyDrive/DermaMNIST/images\"  \n",
    "mask_dir = \"/content/drive/MyDrive/DermaMNIST/masks\"   \n",
    "os.makedirs(mask_dir, exist_ok=True)  \n",
    "\n",
    "train_csv_path = \"/content/drive/MyDrive/DermaMNIST/train_dataset.csv\"\n",
    "val_csv_path = \"/content/drive/MyDrive/DermaMNIST/val_dataset.csv\"\n",
    "test_csv_path = \"/content/drive/MyDrive/DermaMNIST/test_dataset.csv\"\n",
    "\n",
    "train_updated_csv_path = \"/content/drive/MyDrive/DermaMNIST/train_dataset_with_masks.csv\"\n",
    "val_updated_csv_path = \"/content/drive/MyDrive/DermaMNIST/val_dataset_with_masks.csv\"\n",
    "test_updated_csv_path = \"/content/drive/MyDrive/DermaMNIST/test_dataset_with_masks.csv\"\n",
    "\n",
    "def generate_mask_with_opencv(image_path, threshold, mask_dir):\n",
    "\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "\n",
    "    _, mask = cv2.threshold(image, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    mask_filename = os.path.basename(image_path).replace(\".png\", \"_mask.png\")\n",
    "    mask_path = os.path.join(mask_dir, mask_filename)\n",
    "    cv2.imwrite(mask_path, mask)\n",
    "\n",
    "    return mask_path\n",
    "\n",
    "def process_dataset_with_masks(csv_path, updated_csv_path, image_dir, mask_dir, threshold):\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df['mask_path'] = df['image_path'].apply(\n",
    "        lambda img_path: generate_mask_with_opencv(os.path.join(image_dir, os.path.basename(img_path)), threshold, mask_dir)\n",
    "    )\n",
    "\n",
    "    df.to_csv(updated_csv_path, index=False)\n",
    "    print(f\"Updated CSV saved to {updated_csv_path}\")\n",
    "\n",
    "threshold = 110\n",
    "\n",
    "process_dataset_with_masks(train_csv_path, train_updated_csv_path, image_dir, mask_dir, threshold)\n",
    "process_dataset_with_masks(val_csv_path, val_updated_csv_path, image_dir, mask_dir, threshold)\n",
    "process_dataset_with_masks(test_csv_path, test_updated_csv_path, image_dir, mask_dir, threshold)\n",
    "\n",
    "print(f\"Masks generated and paths added to train, val, and test datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPCCRd0gSd_Y",
    "outputId": "c5ff11d6-b666-4b33-8cf5-18f206846836"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 218MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1885171606966622\n",
      "Epoch 2, Loss: 0.07587418446666029\n",
      "Epoch 3, Loss: 0.05629654589734257\n",
      "Epoch 4, Loss: 0.04516453313381841\n",
      "Epoch 5, Loss: 0.039886192125204493\n"
     ]
    }
   ],
   "source": [
    "# Modified Unet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import segmentation_models_pytorch as smp\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def clean_labels(df):\n",
    "    return df\n",
    "\n",
    "train_df = clean_labels(pd.read_csv('/content/drive/MyDrive/DermaMNIST/train_dataset_with_masks.csv'))\n",
    "val_df = clean_labels(pd.read_csv('/content/drive/MyDrive/DermaMNIST/val_dataset_with_masks.csv'))\n",
    "test_df = clean_labels(pd.read_csv('/content/drive/MyDrive/DermaMNIST/test_dataset_with_masks.csv'))\n",
    "\n",
    "def validate_image_paths(df, base_dir):\n",
    "    def check_image_exists(path):\n",
    "        full_path = os.path.join(base_dir, os.path.basename(path))\n",
    "        return full_path if os.path.exists(full_path) else None\n",
    "    df['image_path'] = df['image_path'].apply(check_image_exists)\n",
    "    return df.dropna(subset=['image_path']).reset_index(drop=True)\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/DermaMNIST/images\"\n",
    "train_df = validate_image_paths(train_df, base_dir)\n",
    "val_df = validate_image_paths(val_df, base_dir)\n",
    "test_df = validate_image_paths(test_df, base_dir)\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, base_dir, transforms=None):\n",
    "        self.df = df\n",
    "        self.base_dir = base_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        mask_path = row['mask_path']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "        return image, mask.long()\n",
    "\n",
    "data_transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = SegmentationDataset(train_df, base_dir, transforms=data_transforms)\n",
    "val_dataset = SegmentationDataset(val_df, base_dir, transforms=data_transforms)\n",
    "test_dataset = SegmentationDataset(test_df, base_dir, transforms=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbjRbZIoJwrp"
   },
   "outputs": [],
   "source": [
    "# Standard UNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DermaMNISTClassifier(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def prepare_data(data_dir, batch_size=32):\n",
    "\n",
    "    train_images = np.load(Path(data_dir) / 'train_images.npy')\n",
    "    train_labels = np.load(Path(data_dir) / 'train_labels.npy')\n",
    "    val_images = np.load(Path(data_dir) / 'val_images.npy')\n",
    "    val_labels = np.load(Path(data_dir) / 'val_labels.npy')\n",
    "\n",
    "    train_images = np.transpose(train_images, (0, 3, 1, 2))\n",
    "    val_images = np.transpose(val_images, (0, 3, 1, 2))\n",
    "\n",
    "    train_labels = train_labels.squeeze()\n",
    "    val_labels = val_labels.squeeze()\n",
    "\n",
    "    train_images = torch.from_numpy(train_images).float()\n",
    "    train_labels = torch.from_numpy(train_labels).long()\n",
    "    val_images = torch.from_numpy(val_images).float()\n",
    "    val_labels = torch.from_numpy(val_labels).long()\n",
    "\n",
    "    train_dataset = TensorDataset(train_images, train_labels)\n",
    "    val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}')):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_derma_model.pth')\n",
    "            print('Saved best model!')\n",
    "\n",
    "        print('-' * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = DermaMNISTClassifier(n_channels=3, n_classes=7)\n",
    "\n",
    "    data_dir = './medmnist_data/dermamnist'\n",
    "    train_loader, val_loader = prepare_data(data_dir, batch_size=32)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, num_epochs=100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py_Z6UosTQvR"
   },
   "outputs": [],
   "source": [
    "# Accuracy and AUC\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_true_labels = []\n",
    "all_predicted_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        all_true_labels.extend(masks.cpu().numpy().flatten())\n",
    "        all_predicted_probs.extend(probs)\n",
    "\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_predicted_probs = np.array(all_predicted_probs)\n",
    "\n",
    "num_classes = all_predicted_probs.shape[1]\n",
    "all_true_labels_one_hot = label_binarize(all_true_labels, classes=list(range(num_classes)))\n",
    "\n",
    "predicted_classes = np.argmax(all_predicted_probs, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, predicted_classes)\n",
    "roc_auc = roc_auc_score(all_true_labels_one_hot, all_predicted_probs, multi_class=\"ovr\", average=\"macro\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHMkIkJZUJj_",
    "outputId": "276ccbc6-593d-4c10-fc9f-ab4a9abea516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to unet_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "save_path = \"unet_model.pth\" \n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
